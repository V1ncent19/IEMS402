\documentclass[11pt,a4paper]{ctexart}
%以下为所使用的宏包
\usepackage{ulem}%下划线
\usepackage{amsmath,amsfonts,amssymb,amsthm,amsbsy}%数学符号
\usepackage{graphicx}%插入图片
\usepackage{booktabs}%三线表
%\usepackage{indentfirst}%首行缩进
\usepackage{tikz}%作图
\usepackage{appendix}%附录
\usepackage{array}%多行公式/数组
\usepackage{makecell}%表格缩并
\usepackage{siunitx}%SI单位--\SI{number}{unit}
\usepackage{mathrsfs}%数学字体
\usepackage{enumitem}%列表间距
\usepackage{multirow}%列表横向合并单元格
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green]{hyperref}%超链接引用
\usepackage{float}%图片、表格位置排版
\usepackage{pict2e,keyval,fp,diagbox}%带有斜线的表格
\usepackage{fancyvrb,listings}%设置代码插入环境
\usepackage{minted}%代码环境设置
\usepackage{fontspec}%字体设置
\usepackage{color,xcolor}%颜色设置
\usepackage{titlesec} %自定义标题格式
\usepackage{tabularx}%列表扩展
\usepackage{authblk}%titlepage作者信息
\usepackage{nicematrix}%更好的矩阵标定
\usepackage{fbox}%更多浮动体盒子



%以下是页边距设置
\usepackage[left=0.5in,right=0.5in,top=0.81in,bottom=0.8in]{geometry}

%以下是段行设置
\linespread{1.4}%行距
\setlength{\parskip}{0.1\baselineskip}%段距
\setlength{\parindent}{2em}%缩进


%其他设置
\numberwithin{equation}{section}%公式按照章节编号
\newenvironment{point}{\raggedright$\blacktriangleright$}{}
\newenvironment{algorithm}[1]{\vspace{12pt} \hrule\hrule \vspace{3pt} \noindent\textbf{\color[HTML]{E63F00}Algorithm } \,\textit{#1} \vspace{3pt} \hrule\vspace{6pt}}{\vspace{6pt}\hrule\hrule \vspace{12pt}} % 算法伪代码格式环境


%代码环境\lst设置
\definecolor{CodeBlue}{HTML}{268BD2}
\definecolor{CodeBlue2}{HTML}{0000CD}
\definecolor{CodeGreen}{HTML}{2AA1A2}
\definecolor{CodeRed}{HTML}{CB4B16}
\definecolor{CodeYellow}{HTML}{B58900}
\definecolor{CodePurPle}{HTML}{D33682}
\definecolor{CodeGreen2}{HTML}{859900}
\lstset{
    basicstyle=\tt,%字体设置
    numbers=left, %设置行号位置
    numberstyle=\tiny\color{black}, %设置行号大小
    keywordstyle=\color{black}, %设置关键字颜色
    stringstyle=\color{CodeRed}, %设置字符串颜色
    commentstyle=\color{CodeGreen}, %设置注释颜色
    frame=single, %设置边框格式
    escapeinside=`, %逃逸字符(1左面的键)，用于显示中文
    %breaklines, %自动折行
    extendedchars=false, %解决代码跨页时，章节标题，页眉等汉字不显示的问题
    xleftmargin=2em,xrightmargin=2em, aboveskip=1em, %设置边距
    tabsize=4, %设置tab空格数
    showspaces=false, %不显示空格
    emph={TRUE,FALSE,NULL,NAN,NA,<-,},emphstyle=\color{CodeBlue2}, %其他高亮}
}


%节标题格式设置
\titleformat{\section}[block]{\large\bfseries}{Exercise \arabic{section}}{1em}{}[]
\titleformat{\subsection}[block]{}{    \arabic{section}.(\alph{subsection})}{0.3em}{}[]
\titleformat{\subsubsection}[block]{\normalsize}{    \arabic{section}.(\alph{subsection}).(\roman{subsubsection})}{0.3em}{}[]
% \titleformat{\paragraph}[block]{\small\bfseries}{[\arabic{paragraph}]}{1em}{}[]


% \titleformat{\sectioncommand}[shape]{format}{title-label}{sep}{before-title}[after-title]



% 中文字号
% 初号42pt, 小初36pt, 一号26pt, 小一24pt, 二号22pt, 小二18pt, 三号16pt, 小三15pt, 四号14pt, 小四12pt, 五号10.5pt, 小五9pt


\begin{document}

\begin{center}\thispagestyle{plain}

{\LARGE\textbf{IEMS 402 Statistical Learning - 2025 Winter}}

{\Large\textbf{HW3}}

Tuorui Peng\footnote{TuoruiPeng2028@u.northwestern.edu}
\end{center}

\thispagestyle{myheadings}\markright{Compiled using \LaTeX}
\pagestyle{myheadings}\markright{Tuorui Peng}




\section{Ensemble and Bias-Variance Trade-off}

\subsection{Weight Average or Prediction Average?}


Note that weight average \& prediction average are both linear functions of the predictions. Thus, the two are equivalent if we are using linear models (do not expending the model space), while for non-linear models like nn, the two are not equivalent (new model space).

\subsection{Bagging - Uncorrelated Models}

\subsubsection{Bias with bagging}

By total expectation law, we have
\begin{align*}
    \mathbb{E}_{\mathcal{D}}\left[ \mathbb{E}_{\mathcal{D}_i}\left[ h(x;\mathcal{D}_i)|x \right]   \right]  =& \mathbb{E}_{\mathcal{D}}\left[  h(x;\mathcal{D}_i)|x \right]   
\end{align*}
Thus we have
\begin{align*}
    \mathrm{ bias }=& \mathbb{E}\left[ \big(\mathbb{E}\left[ \bar{h}(x;\mathcal{D})|x \right]  - y_*(x)\big)^2 \right] \\
    =&   \mathbb{E}\left[ \big(\mathbb{E}\left[ k^{-1}\sum_{i=1}^k h(x;\mathcal{D}_i)  |x \right]  - y_*(x)\big)^2 \right] \\
    =&  \mathbb{E}\left[ \big(\mathbb{E}\left[ h(x;\mathcal{D})|x \right]  - y_*(x)\big)^2 \right] 
\end{align*}

\subsubsection{Variance with bagging}

With uncorrelated assumption, we have
\begin{align*}
    \mathrm{ variance }=& \mathbb{E}\left[ \big( \bar{h}(x;\mathcal{D}) - \mathbb{E}\left[ \bar{h}(x;\mathcal{D})|x \right] \big)^2 \right] \\
    =&   \mathbb{E}\left[ \big( k^{-1}\sum_{i=1}^k h(x;\mathcal{D}_i) - \mathbb{E}\left[ h(x;\mathcal{D})|x \right] \big)^2 \right] \\
    =& k^{-2} \sum_{i=1}^k \mathbb{E}\left[ \big( h(x;\mathcal{D}_i) - \mathbb{E}\left[ h(x;\mathcal{D})|x \right] \big)^2 \right] \\
    &+ k^{-2} \sum_{i\neq j}  \underbrace{\mathbb{E}\left[ \big( h(x;\mathcal{D}_i) - \mathbb{E}\left[ h(x;\mathcal{D})|x \right] \big)\big( h(x;\mathcal{D}_j) - \mathbb{E}\left[ h(x;\mathcal{D})|x \right] \big) \right] }_{0} \\
    =& k^{-1} \sigma ^2  
\end{align*}


\subsection{Bagging- General Case}

\subsubsection{Bias under Correlation}

Note that in the derivation of bias, we only utilized the total expectation law, thus the bias term remains the same.

\subsubsection{Variance under Correlation}
Turn to correlated case with $ corr\big(h(x;\mathcal{D}_i), h(x;\mathcal{D}_j)\big) = \delta _{ij}(1-\rho )+\rho   $, we have 
\begin{align*}
    \mathrm{ variance }=& \mathbb{E}\left[ \big( \bar{h}(x;\mathcal{D}) - \mathbb{E}\left[ \bar{h}(x;\mathcal{D})|x \right] \big)^2 \right] \\
    =&   \mathbb{E}\left[ \big( k^{-1}\sum_{i=1}^k h(x;\mathcal{D}_i) - \mathbb{E}\left[ h(x;\mathcal{D})|x \right] \big)^2 \right] \\
    =& k^{-2} \sum_{i=1}^k \mathbb{E}\left[ \big( h(x;\mathcal{D}_i) - \mathbb{E}\left[ h(x;\mathcal{D})|x \right] \big)^2 \right] \\
    &+ k^{-2} \sum_{i\neq j}  \mathbb{E}\left[ \big( h(x;\mathcal{D}_i) - \mathbb{E}\left[ h(x;\mathcal{D})|x \right] \big)\big( h(x;\mathcal{D}_j) - \mathbb{E}\left[ h(x;\mathcal{D})|x \right] \big) \right]  \\
    =& k^{-1} \sigma ^2  + \dfrac{ k-1 }{ k }\sigma ^2 \rho \\
    =& \sigma ^2\big( \rho + \dfrac{ 1-\rho  }{ k }  \big) 
\end{align*}

\subsubsection{Intuitions on bagging}

\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item Generall we should have a positive $ \rho  $ due to the similarity between boostrapped models, thus the variance term decreases towards $ \rho \sigma ^2 $ as $ k $ increases. i.e. more ensembles could reduce the variance, but we would still have variance induced by the intrinsic correlation.
    \item $ \rho =0 $ case: variance term is $ \sigma ^2/k $, which could reduce to $ 0 $ as $ k $ increases.
    \item $ \rho =1 $ case: variance term is $ \sigma ^2 $, this case bagging does not help to reduce the variance.
\end{itemize}

\section{Central Limit Theorem for Kernel Density Estimator}

\subsection{}

Kernel density estimator is defined as
\begin{align*}
    \hat{p}_h(x) = \dfrac{1}{n}\sum_{i=1}^n \dfrac{ 1 }{ h }  K_h(\dfrac{ \left\Vert x-X_i \right\Vert  }{ h } )
\end{align*}

\begin{enumerate}[topsep=2pt,itemsep=2pt]
    \item For any $ q>1 $, we have
    \begin{align*}
        \mathbb{E}\left[ \left\vert \dfrac{ 1 }{ h }  K_h(\dfrac{ \left\Vert x-X_i \right\Vert  }{ h } ) - p_h(x) \right\vert ^q \right] \begin{cases}
            \geq & \dfrac{ 1 }{ 2^q }\left[ \mathbb{E}\left[ \left\vert \dfrac{ 1 }{ h }  K_h(\dfrac{ \left\Vert x-X_i \right\Vert  }{ h } ) \right\vert ^q  \right] - \mathbb{E}\left[ \left\vert p_h(x) \right\vert ^q \right]  \right] \\
            \leq & \dfrac{ 1 }{ 2^q }\left[ \mathbb{E}\left[ \left\vert \dfrac{ 1 }{ h }  K_h(\dfrac{ \left\Vert x-X_i \right\Vert  }{ h } ) \right\vert ^q  \right] + \mathbb{E}\left[ \left\vert p_h(x) \right\vert ^q \right]  \right]
        \end{cases}
    \end{align*}
    in which
    \begin{align*}
        \mathbb{E}\left[ \left\vert \dfrac{ 1 }{ h }  K_h(\dfrac{ \left\Vert x-X_i \right\Vert  }{ h } ) \right\vert ^q  \right] =& \dfrac{ 1 }{ h^{q-1} } \int \left\vert K(\left\Vert v \right\Vert ) \right\vert ^q p(x + hv) \,\mathrm{d}v = \Theta(h^{-q}) \\
        \mathbb{E}\left[ \left\vert p_h(x) \right\vert ^q \right] =& \left\vert \mathbb{E}\left[ \hat{p}_h(x) \right]  \right\vert ^q = \Theta(n^{-q}h^{-q}) 
    \end{align*}
    as a result, taking $ q=2+\delta  $ we have
    \begin{align*}
        \sum_{i=1}^n \mathbb{E}\left[ \left\vert \dfrac{ 1 }{ h }  K_h(\dfrac{ \left\Vert x-X_i \right\Vert  }{ h } ) - p_h(x) \right\vert ^{2+\delta } \right]     = \Theta( n h^{-1-\delta } + n^{-1-\delta }h^{-1-\delta }) = \Theta( n h^{-1-\delta })
    \end{align*}
    \item Taking $ q = 2 $ we have
    \begin{align*}
        \big(\sum_{i=1}^n \sigma _i^2(x))^{2+\delta }=& \Theta ((nh^{-1}))^{(2+\delta )/2}
    \end{align*}
    \item Now we verify the condition for Lyaupnov CLT, we have
    \begin{align*}
        \lim_{n\to\infty } \dfrac{ \sum_{i=1}^n \mathbb{E}\left[ \left\vert \dfrac{ 1 }{ h }  K_h(\dfrac{ \left\Vert x-X_i \right\Vert  }{ h } ) - p_h(x) \right\vert ^q \right] }{ (\sum_{i=1}^n \sigma _i^2(x))^{(2+\delta /2)} }  =& \lim_{n\to\infty } \dfrac{ \Theta( n h^{-1-\delta }) }{ \Theta ((nh^{-1}))^{(2+\delta )/2} } \\
        =&  \lim_{n\to\infty }\Theta ( n^{-\delta /2}  h ^{-\delta /2} ) = 0
    \end{align*}
    thus we have the Lyaupnov CLT for KDE as 
    \begin{align*}
        \dfrac{ \hat{p}_h(x)- p_h(x) }{ s_n(x) } \xrightarrow[]{\mathrm{d}} N(0,1)  
    \end{align*}
    in which $ s_n^2(x) = var(\hat{p}_h(x)) $.

    
    
    
    
\end{enumerate}

\subsection{}

\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item Variance term:
    \begin{align*}
         s_n^2(x) = \Theta( n^{-1}h_n^{-1} )
    \end{align*}
    \item Bias term:
    \begin{align*}
         \left\vert p_{h_n}(x)-p(x) \right\vert =& \left\vert \int K(\left\Vert v \right\Vert ) \big(p(x+h_nv) - p_{x,\beta }(x+h_nv)\big) \,\mathrm{d}v \right\vert \\
         \sim& Lh_n^\beta \int K(s)\left\Vert s \right\Vert ^\beta \,\mathrm{d}s  := ch_n^\beta \quad \text{generally speaking.}
    \end{align*}
    \item Optimal $ h_n $: choosing $ \ell_2 $ risk as the criterion, we have
    \begin{align*}
        \mathrm{ error } \asymp \mathrm{ bias }^2 + \mathrm{ variance } = \Theta( h_n^{2\beta } ) + \Theta( n^{-1}h_n^{-1} )
    \end{align*}
    which gives the optimal $ h_n $ as 
    \begin{align*}
        h_n^* = \Theta( n^{-1/(2\beta +1)} ) 
    \end{align*}
    
\end{itemize}

As a result, at $ \beta =2 $ we have $ h_n^* = \Theta (n^{-1/5}) $ and
\begin{align*}
    \dfrac{ \left\vert p_{h_n}(x)-p(x) \right\vert }{ s_n(x) } = \Theta( n^{1/2}h_n^{5/2} ) = \Theta(1) 
\end{align*}
say the $ \Theta(1) $ limit is $ b(x) $, which gives


\begin{align*}
    \dfrac{ \hat{p}_h(x)- p(x) }{ s_n(x) } = \dfrac{ \hat{p}_h(x)- p_h(x) }{ s_n(x) } + \dfrac{ p_h(x)- p(x) }{ s_n(x) } \xrightarrow[]{\mathrm{d}} N(0,1) + b(x) = N(b(x),1)
\end{align*}

\section{Estimating the Sobolev Ellipsoid via Spectral Methods}

\subsection{}
By the basis expansion, we have
\begin{align*}
    \mathrm{ risk } = & \mathbb{E}\left[ \int_0^1 (\hat{p}(x) - p(x))^2 \,\mathrm{d}x \right]   \\
    =& \mathbb{E}\left[ \int_0^1 \big( \sum_{j=1}^k \left( \hat{\beta }_j - \beta_j  \right) \phi _j(x) - \sum_{j=k+1}^\infty \beta _j(x) \big)^2 \,\mathrm{d}x \right] \\
    =& \sum_{j=1}^k \mathbb{E}\left[ \left( \hat{\beta }_j - \beta_j  \right)^2 \right] + \sum_{j=k+1}^\infty \beta _j^2\\
    =& \sum_{j=1}^k \mathbb{E}\left[ \left( n^{-1}\sum_{i=1}^n \phi _j(X_i) - \beta_j  \right)^2 \right] + \sum_{j=k+1}^\infty \beta _j^2
\end{align*}
Now note that we have
\begin{align*}
    \mathbb{E}\left[ \phi _j(X_i) \right] =& \int_0^1 \phi _j(x)p(x) \,\mathrm{d}x = \beta _j
\end{align*}
thus we have
\begin{align*}
    \mathrm{ risk } =&\sum_{j=1}^k \mathbb{E}\left[ \left( n^{-1}\sum_{i=1}^n \phi _j(X_i) - \beta_j  \right)^2 \right] + \sum_{j=k+1}^\infty \beta _j^2\\
    =& \sum_{j=1}^k \left[\dfrac{ 1 }{ n^2 }\sum_{i=1}^n \mathbb{E}\left[ \phi _j(X_i)^2 \right] - 2\beta _j \dfrac{ 1 }{ n }\sum_{i=1}^n \beta _j + \beta _j^2 \right] + \sum_{j=k+1}^\infty \beta _j^2\\
    \leq &  \sum_{j=1}^k \left[ \dfrac{ C^2 }{ n } - \beta _j^2 \right] + \sum_{j=k+1}^\infty \beta _j^2\\
    \leq & \dfrac{ ck }{ n } + \sum_{j=k+1}^\infty \beta _j^2  .
\end{align*}

\subsection{}
We have for Sobolev ellipsoid $ E(m,L) $ that
\begin{align*}
    k^{2m}\sum_{j=k+1}^\infty \beta _j^2 \leq &\sum_{j=1}^k \beta _j^2 j^{2m} + \sum_{j=k+1}^\infty \beta _j^2 j^{2m} < L^2  \Rightarrow \sum_{j=k+1}^\infty \beta _j^2 < \dfrac{ L^2 }{ k^{2m} } 
\end{align*}
as a result we have
\begin{align*}
    \mathrm{ risk } \leq & \dfrac{ ck }{ n } + \dfrac{ L^2 }{ k^{2m} } \lesssim \dfrac{ k }{ n }+ \left( \dfrac{ 1 }{ k } \right)^{2m}
\end{align*}

The optimal $ k $ is obtained at $ k\asymp n^{1/(2m+1)} $, which gives the optimal risk $ \lesssim  n^{-2m/(2m+1)} $.














    


    





    




















\end{document}